{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Language Detection Model Training\n",
    "This notebook trains a model to recognize American Sign Language (ASL) gestures.\n",
    "The model works during specified time periods (6 PM to 10 PM) as per requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Setup\n",
    "Using ASL Alphabet dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASL alphabet letters (A-Z)\n",
    "asl_letters = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
    "               'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2' '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "all_classes = asl_letters \n",
    "\n",
    "# Create dataset structure\n",
    "dataset_path = '../datasets/sign_language_data'\n",
    "\n",
    "for class_name in all_classes:\n",
    "    os.makedirs(f'{dataset_path}/train/{class_name}', exist_ok=True)\n",
    "    os.makedirs(f'{dataset_path}/val/{class_name}', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-based Operation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current operational status: False\n"
     ]
    }
   ],
   "source": [
    "def is_operational_time():\n",
    "    \"\"\"\n",
    "    Check if current time is between 6 PM and 10 PM\n",
    "    \"\"\"\n",
    "    current_time = datetime.now().time()\n",
    "    start_time = datetime.strptime('18:00', '%H:%M').time()  # 6 PM\n",
    "    end_time = datetime.strptime('22:00', '%H:%M').time()    # 10 PM\n",
    "    \n",
    "    return start_time <= current_time <= end_time\n",
    "\n",
    "print(f'Current operational status: {is_operational_time()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing with Hand Landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)\n",
    "\n",
    "# Image parameters\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "\n",
    "def extract_hand_landmarks(image):\n",
    "    \"\"\"\n",
    "    Extract hand landmarks from image\n",
    "    \"\"\"\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_image)\n",
    "    \n",
    "    landmarks = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "    \n",
    "    # Pad with zeros if no hands detected\n",
    "    while len(landmarks) < 63:  # 21 landmarks * 3 coordinates\n",
    "        landmarks.append(0.0)\n",
    "    \n",
    "    return np.array(landmarks[:63])  # Take first 63 values\n",
    "\n",
    "# Data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "Using MobileNetV2 for efficient real-time processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " images (InputLayer)         [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 222, 222, 32)         896       ['images[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPoolin  (None, 111, 111, 32)         0         ['conv2d_3[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " landmarks (InputLayer)      [(None, 21, 2)]              0         []                            \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)         (None, 394272)               0         ['max_pooling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " dense_23 (Dense)            (None, 21, 64)               192       ['landmarks[0][0]']           \n",
      "                                                                                                  \n",
      " dense_22 (Dense)            (None, 128)                  5046694   ['flatten_3[0][0]']           \n",
      "                                                          4                                       \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3  (None, 64)                   0         ['dense_23[0][0]']            \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate  (None, 192)                  0         ['dense_22[0][0]',            \n",
      " )                                                                   'global_average_pooling1d_3[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dense_24 (Dense)            (None, 128)                  24704     ['concatenate_5[0][0]']       \n",
      "                                                                                                  \n",
      " dense_25 (Dense)            (None, 36)                   4644      ['dense_24[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 50497380 (192.63 MB)\n",
      "Trainable params: 50497380 (192.63 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Image branch\n",
    "image_input = tf.keras.Input(shape=(224, 224, 3), name=\"images\")\n",
    "x1 = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(image_input)\n",
    "x1 = tf.keras.layers.MaxPooling2D()(x1)\n",
    "x1 = tf.keras.layers.Flatten()(x1)\n",
    "x1 = tf.keras.layers.Dense(128, activation=\"relu\")(x1)\n",
    "\n",
    "# Landmark branch\n",
    "landmark_input = tf.keras.Input(shape=(21, 2), name=\"landmarks\")\n",
    "x2 = tf.keras.layers.Dense(64, activation=\"relu\")(landmark_input)\n",
    "\n",
    "# collapse to 2D\n",
    "x2 = tf.keras.layers.GlobalAveragePooling1D()(x2)   # (batch, 64)\n",
    "# alternative: x2 = tf.keras.layers.Flatten()(x2)   # (batch, 21*64)\n",
    "\n",
    "# Merge\n",
    "merged = tf.keras.layers.Concatenate()([x1, x2])\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(merged)\n",
    "output = tf.keras.layers.Dense(len(set(labels)), activation=\"softmax\")(x)\n",
    "\n",
    "# Build model\n",
    "model = tf.keras.Model(inputs=[image_input, landmark_input], outputs=output)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Data Generator for Multi-input Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignLanguageDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, image_paths, labels, batch_size=32, img_size=(224, 224)):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "\n",
    "        # Create label mapping\n",
    "        self.all_classes = sorted(set(labels))\n",
    "        self.label_to_index = {label: idx for idx, label in enumerate(self.all_classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.image_paths[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_labels = self.labels[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        return self._generate_data(batch_indexes, batch_labels)\n",
    "\n",
    "    def _generate_data(self, batch_paths, batch_labels):\n",
    "        # Allocate memory for this batch\n",
    "        X_images = np.zeros((len(batch_paths), self.img_size[0], self.img_size[1], 3), dtype=np.float32)\n",
    "        X_landmarks = np.zeros((len(batch_paths), 21, 2), dtype=np.float32)  # <-- FIXED\n",
    "        y = np.zeros((len(batch_paths), len(self.all_classes)), dtype=np.float32)\n",
    "\n",
    "        for i, (path, label) in enumerate(zip(batch_paths, batch_labels)):\n",
    "            # Load image\n",
    "            img = cv2.imread(path)\n",
    "            img = cv2.resize(img, self.img_size)\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            X_images[i] = img_rgb / 255.0\n",
    "\n",
    "            # Extract landmarks\n",
    "            landmarks = extract_hand_landmarks(img_rgb)\n",
    "            if landmarks is not None:\n",
    "                landmarks = np.array(landmarks).reshape(-1, 3)[:, :2]  # keep x,y only\n",
    "            else:\n",
    "                landmarks = np.zeros((21, 2))\n",
    "\n",
    "            X_landmarks[i] = landmarks\n",
    "\n",
    "            # One-hot encode label\n",
    "            label_index = self.label_to_index[label]\n",
    "            y[i] = tf.keras.utils.to_categorical(label_index, num_classes=len(self.all_classes))\n",
    "\n",
    "        return [X_images, X_landmarks], y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction functions created.\n"
     ]
    }
   ],
   "source": [
    "def predict_sign_language(frame, model):\n",
    "    \"\"\"\n",
    "    Predict sign language from video frame\n",
    "    Only works during operational hours (6 PM - 10 PM)\n",
    "    \"\"\"\n",
    "    if not is_operational_time():\n",
    "        return {'prediction': 'Model not operational', 'confidence': 0.0, 'operational': False}\n",
    "    \n",
    "    # Preprocess image\n",
    "    img = cv2.resize(frame, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_normalized = np.expand_dims(img_rgb / 255.0, axis=0)\n",
    "    \n",
    "    # Extract landmarks\n",
    "    landmarks = extract_hand_landmarks(frame)\n",
    "    landmarks_batch = np.expand_dims(landmarks, axis=0)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict([img_normalized, landmarks_batch])\n",
    "    predicted_class = np.argmax(predictions[0])\n",
    "    confidence = float(np.max(predictions[0]))\n",
    "    \n",
    "    return {\n",
    "        'prediction': all_classes[predicted_class],\n",
    "        'confidence': confidence,\n",
    "        'operational': True\n",
    "    }\n",
    "\n",
    "def draw_hand_landmarks(image, results):\n",
    "    \"\"\"\n",
    "    Draw hand landmarks on image\n",
    "    \"\"\"\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "    return image\n",
    "\n",
    "print('Prediction functions created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 2515\n",
      "Unique labels: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Sample path: C:\\Users\\sarva\\Emotion_detection-main\\datasets\\sign_language_data\\train\\0\\hand1_0_bot_seg_1_cropped.jpeg\n",
      "Sample label: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Path to your dataset root folder\n",
    "train_dir = r\"C:\\Users\\sarva\\Emotion_detection-main\\datasets\\sign_language_data\\train\"\n",
    "\n",
    "paths = []\n",
    "labels = []\n",
    "\n",
    "# Loop through each subfolder (each class)\n",
    "for folder in os.listdir(train_dir):\n",
    "    folder_path = os.path.join(train_dir, folder)\n",
    "    if os.path.isdir(folder_path):  \n",
    "        for file in glob.glob(os.path.join(folder_path, \"*.*\")):  \n",
    "            paths.append(file)\n",
    "            labels.append(folder)  # label is the folder name\n",
    "\n",
    "print(\"Total images:\", len(paths))\n",
    "print(\"Unique labels:\", sorted(set(labels)))\n",
    "print(\"Sample path:\", paths[0] if paths else \" No images found\")\n",
    "print(\"Sample label:\", labels[0] if labels else \" No labels found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final landmarks shape: (21, 2)\n",
      "First 5 points:\n",
      " [[0.30983633 0.33478421]\n",
      " [0.34160697 0.32744119]\n",
      " [0.38675517 0.29366809]\n",
      " [0.4496811  0.27904955]\n",
      " [0.50699651 0.28079921]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Pick one image path from your dataset\n",
    "sample_img_path = paths[0]\n",
    "\n",
    "# Load image\n",
    "img = cv2.imread(sample_img_path)\n",
    "\n",
    "# Extract landmarks\n",
    "landmarks = extract_hand_landmarks(img)\n",
    "\n",
    "if landmarks is not None:\n",
    "    landmarks = np.array(landmarks).reshape(21, 3)[:, :2]\n",
    "else:\n",
    "    landmarks = np.zeros((21, 2))\n",
    "\n",
    "print(\"Final landmarks shape:\", landmarks.shape)\n",
    "print(\"First 5 points:\\n\", landmarks[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final X_landmarks shape: (1, 21, 2)\n",
      "First 5 points:\n",
      " [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Pick one test image\n",
    "img_path = paths[0]\n",
    "img = cv2.imread(img_path)\n",
    "img = cv2.resize(img, (224, 224))\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Extract landmarks\n",
    "landmarks = extract_hand_landmarks(img_rgb)\n",
    "if landmarks is not None:\n",
    "    landmarks = np.array(landmarks).reshape(-1, 3)[:, :2]  # x,y only\n",
    "else:\n",
    "    landmarks = np.zeros((21, 2))\n",
    "\n",
    "# Allocate a placeholder for testing\n",
    "X_landmarks = np.zeros((1, 21, 2))  \n",
    "X_landmarks[0] = landmarks\n",
    "\n",
    "print(\"Final X_landmarks shape:\", X_landmarks.shape)\n",
    "print(\"First 5 points:\\n\", X_landmarks[0][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\kinshu\\envs\\mediapipe\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 157s 2s/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.1151 - val_accuracy: 0.9622 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "63/63 [==============================] - 157s 2s/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.1117 - val_accuracy: 0.9642 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "63/63 [==============================] - 152s 2s/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.1084 - val_accuracy: 0.9642 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "63/63 [==============================] - 163s 3s/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1107 - val_accuracy: 0.9662 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "63/63 [==============================] - 134s 2s/step - loss: 9.0204e-04 - accuracy: 1.0000 - val_loss: 0.1114 - val_accuracy: 0.9662 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "63/63 [==============================] - 133s 2s/step - loss: 6.6685e-04 - accuracy: 1.0000 - val_loss: 0.1137 - val_accuracy: 0.9622 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "63/63 [==============================] - 132s 2s/step - loss: 5.3153e-04 - accuracy: 1.0000 - val_loss: 0.1121 - val_accuracy: 0.9642 - lr: 5.0000e-04\n",
      "Epoch 8/25\n",
      "63/63 [==============================] - 133s 2s/step - loss: 4.6815e-04 - accuracy: 1.0000 - val_loss: 0.1125 - val_accuracy: 0.9622 - lr: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split your dataset\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Create generators\n",
    "train_generator = SignLanguageDataGenerator(train_paths, train_labels, batch_size=32)\n",
    "val_generator = SignLanguageDataGenerator(val_paths, val_labels, batch_size=32)\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3),\n",
    "    tf.keras.callbacks.ModelCheckpoint('sign_language_model.h5', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train with validation\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=25,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "model.save('sign_language_detection_model_finals.h5')\n",
    "print('Model saved successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
